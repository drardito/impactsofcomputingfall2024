## Feedback from Week 8 Discussion

![boy at microphone](https://images.unsplash.com/photo-1453738773917-9c3eff1db985?q=80&w=1770&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D)

I have had the pleasure of reading through your discussion from Week 7, where you used the evidence from notifications, etc., to speculate on who your computing systems understood you to be. Here is some feedback about your work in that discussion.

1. ***Engagement/Interaction/Substance***. So much of what you discussed had to with imbalances of power and the impact exerted on us. Here are some highlights:

   

   ***Omnipresent algorithms***
   @nivard -  What I found interesting is everything from security systems to thermostats is controlled by algorithms that learn your routines to maximize safety and energy efficiency. Although this is very helpful in today's day and age  It also raises concerns over data security and privacy, notwithstanding its convenience.

   @amelillo2 -  As I was completing the notes for the reading and listening, I started to realize that everything done in my life is affected by some sort of algorithm.

   @pisapiat During week 9s readings and audio sessions, about Algorithms being imperceptible, to humans makes us realize that we don't entirely grasp their workings but acknowledge them as directives steering computers in tasks execution and data processing or decision making processes.

   @emilyramon  People fail to realize that the manner in which algorithms are set up they're bound to be shown what they want to see. This is made even easier with many apps giving users the "not interested" option to further cater to what they want to see. 

   @custardoliver3 Before I took this class I always wondered how advertising and emails that seemed to be tailored to me came to my inbox. I guess since I had no idea at the time about how pervasive and interconnected computer based algorithm are, I just thought maybe it was a coincidence. Knowing what I have learned in this course so far, it is incredible to think that the all of this coincidence is instead purposeful targeted online activity directed to exploit and utilize my hobbies and interests so that a company can generate revenue using mw.

   

2. ***Visualizations.*** As you now know, I like to use computational tools to *analyze* these types of discussions. I think in a course of the Impacts of Computing, it makes sense (and for possibly interesting discussion) to use these types of computational tools. For this week (and throughout the course), I used/will use [Voyant Tools](https://voyant-tools.org/), which is a powerful tool designed to visualize various features of text. Here are some highlights of this analysis.

   1. *Features of your writing.*  By looking at these features of your writing, it is possible to learn some high level characteristics of your work. ***Volume of writing*** is a measurement of the quantity of your collective writing. For this week, you collectively used about 5700 words, which represents a decrease from the previous Mattermost discussion (about 6700 words). This makes sense because in part you were referring to posts from last week.  Additionally, about 1300 of the terms you used were unique, which is about the same as the last round. ***Vocabulary density*** is a measure of how rich or varied the vocabulary is in a given text or corpus. A vocabulary density of 0.225means that, on average, about 23% of the words in the corpus are unique. This is higher than last time, and this makes sense since many of you were talking about the same kinds of notifications/evidence (e.g., *ads*, *notifications*, and *systems*).

      ![summary of features of writing for this week](https://github.com/drardito/impactsofcomputingfall2024/blob/main/Images/Impacts%20FA24%20Week8%20Features.png?raw=true)

   2. *Word Frequencies*. This word cloud represents the relative frequencies of the words you used in your posts. As mentioned above, the most frequently used (largest) terms reflect on the common set of notifications/evidence you cited in your post: *algorithms*, *social*, *media*, and *pm* (no surprise, there). As usual, we see the relatively high frequency of the word *pm*, which is taken from the date/time stamp in Mattermost. 

      ![wordcloud](https://github.com/drardito/impactsofcomputingfall2024/blob/main/Images/Impacts%20FA24%20Week8%20Wordcloud.png?raw=true)

   3. *Relationships Between Words.* As usual, I performed a ***collocation analysis***, in which we can look at the relationships between works used in your Mattermost posts. In this visualization, thicker grey lines indicate a higher number of connections between terms. I invite you to explore a [live version of this analysis]([Voyant Tools](https://voyant-tools.org/?corpus=12cdc70b376f12f8cb05c6ab48ca7691&query=algorithms&query=social&query=lives&query=reading&query=impact&query=daily&query=media&query=platforms&query=videos&query=think&query=like&query=things&query=pm&query=filter&query=emilyramon&context=7&view=CollocatesGraph)), which will allow you to explore these connections in interesting ways. Note the range of connected terms here, which is much larger than usual. And note the callout to @emilyramon who is frequently connected to other terms. Go, Emily!  Please do play with the live version. I think you will be surprised by the connections you see there.

      ![word link chart](https://github.com/drardito/impactsofcomputingfall2024/blob/main/Images/Impacts%20FA24%20Week8%20Collocations.png?raw=true)

4. Topic Modeling. For the first time this semester, I used Voyant-Tools to do what is called *topic modeling*, which it defines this way: *The Topics tool is designed to help you understand what topics (term clusters) exist and how they are distributed. To simplify, words in each document are randomly assigned to a specified number of topics (you can determine the number of topics). The algorithm then goes through a number of iterations (50) and tries to refine the model of which terms are best suited to which topics (based on co-occurrence in the documents). It's important to understand that this algorithm starts by randomly assigning words to topics and so every time topic modelling is run you are likely to get different results. Each topic technically contains every word in the corpus, but only the top 10 words are displayed. The order of the words is important and the first words likely contribute much more to the topic than the latter words.*

   ![diagram of a topic model](https://github.com/drardito/impactsofcomputingfall2024/blob/main/Images/Impacts%20FA24%20Week8%20Topics.png?raw=true)

There are some components of this analysis which seem very coherent with the others, and some not so much. I am very eager to hear your responses to this type of analysis.

Please share your thoughts about any/all of this in the Week 8 discussion on Mattermost.
