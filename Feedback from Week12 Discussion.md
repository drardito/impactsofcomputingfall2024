## Feedback from Week 12 Discussion

![boy at microphone](https://images.unsplash.com/photo-1453738773917-9c3eff1db985?q=80&w=1770&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D)

I have had the pleasure of reading through your discussion from Week 12, where you used the evidence from notifications, etc., to speculate on who your computing systems understood you to be. Here is some feedback about your work in that discussion.

1. ***Engagement/Interaction/Substance***. So much of what you discussed had to with imbalances of power and the impact exerted on us. Here are some highlights:

   

   ***Inequality***

   @nivard However, these technologies have the potential to violate privacy and be prejudiced, unfairly targeting particular groups. Because it can be inaccurate and result in errors in the law enforcement setting, facial recognition is particularly contentious. 

   @custardoliver3 When one reflects more on the uses of AI, however, it become evident that the use of AI can cause problems because it can not be accurate always and has the ability to be prejudice and lacks the human ability to discern special characteristics, which is invaluable to the process as a whole. 

   @pisapiat I've noted in my observations of justice matters concerning recognition methods used by law enforcement agencies when handling suspects from a distance to expedite case resolutions and minimize paperwork diligently rely on facial recognition algorithms to scan and compare features with a database of mugshots and unsolved images for scrutiny purposes; I find myself questioning the dependability of this technology, in law enforcement contexts. 

   @carlos9018 For example, I see that people like to use AI like Chatgpt to help them in school or even in everyday life and although it can be helpful for the most part, it will start to affect students or people because the interaction between people will lessen and the learning will just go out the window as well because they have these bits of technology that will just teach it for you and sometimes that isn't the best thing to do.

   @wilmerroldan s. For example, the article on The Access Doctrine explains that there is a mistaken belief that simply giving access to digital technology will solve social inequalities, when in reality this can ignore deeper problems, such as the lack of basic resources.

   

2. ***Visualizations.*** As you now know, I like to use computational tools to *analyze* these types of discussions. I think in a course of the Impacts of Computing, it makes sense (and for possibly interesting discussion) to use these types of computational tools. For this week (and throughout the course), I used/will use [Voyant Tools](https://voyant-tools.org/), which is a powerful tool designed to visualize various features of text. Here are some highlights of this analysis.

   1. *Features of your writing.*  By looking at these features of your writing, it is possible to learn some high level characteristics of your work. ***Volume of writing*** is a measurement of the quantity of your collective writing. For this week, you collectively used about 5800 words, which represents a big increase from the previous Mattermost discussion (about 4700 words). This week, most of you had a great deal to say about facial recognition and inequality. Additionally, about 1300 of the terms you used were unique, which is an increase over last week's discussions. ***Vocabulary density*** is a measure of how rich or varied the vocabulary is in a given text or corpus. A vocabulary density of 0.226 means that, on average, about 23% of the words in the corpus are unique. This is slightly higher than last time, and this makes sense since many of you were talking about the same kinds of global impacts of data privacy. 

      ![summary of features of writing for this week](https://github.com/drardito/impactsofcomputingfall2024/blob/main/Images/Impacts%20FA24%20Week11%20Features.png?raw=true)

   2. *Word Frequencies*. This word cloud represents the relative frequencies of the words you used in your posts. As mentioned above, the most frequently used (largest) terms reflect on the common set of notifications/evidence you cited in your post: *students*, *technology*, *ai*, and *people. As usual, we see the relatively high frequency of the word *pm*, which is taken from the date/time stamp in Mattermost. 

      ![wordcloud](https://github.com/drardito/impactsofcomputingfall2024/blob/main/Images/Impacts%20FA24%20Week11%20Wordcloud.png?raw=true)

   3. *Relationships Between Words.* As usual, I performed a ***collocation analysis***, in which we can look at the relationships between works used in your Mattermost posts. In this visualization, thicker grey lines indicate a higher number of connections between terms. I invite you to explore a [live version of this analysis]https://voyant-tools.org/?corpus=fd278d796b41dbeeb836aaafae6e6faa&query=ai&query=healthcare&query=used&query=help&query=health&query=systems&query=video&query=intelligence&query=impact&query=artificial&query=pm&query=november&query=agree&query=wilmerroldan&query=pisapiat&context=7&view=CollocatesGraph), which will allow you to explore these connections in interesting ways. Note the range of connected terms here, which is reflective of the focus fairness and inequality in this week's texts. Please do play with the live version. I think you will be surprised by the connections you see there.

      ![word link chart](https://github.com/drardito/impactsofcomputingfall2024/blob/main/Images/Impacts%20FA24%20Week11%20Collocations.png?raw=true)

4. Topic Modeling. For the first time this semester, I used Voyant-Tools to do what is called *topic modeling*, which it defines this way: *The Topics tool is designed to help you understand what topics (term clusters) exist and how they are distributed. To simplify, words in each document are randomly assigned to a specified number of topics (you can determine the number of topics). The algorithm then goes through a number of iterations (50) and tries to refine the model of which terms are best suited to which topics (based on co-occurrence in the documents). It's important to understand that this algorithm starts by randomly assigning words to topics and so every time topic modelling is run you are likely to get different results. Each topic technically contains every word in the corpus, but only the top 10 words are displayed. The order of the words is important and the first words likely contribute much more to the topic than the latter words.*

   ![diagram of a topic model](https://github.com/drardito/impactsofcomputingfall2024/blob/main/Images/Impacts%20FA24%20Week10%20Topics.png?raw=true)

There are some components of this analysis which seem very coherent with the others, and some not so much. I am very eager to hear your responses to this type of analysis.

Please share your thoughts about any/all of this in the Week 12 discussion on Mattermost.
